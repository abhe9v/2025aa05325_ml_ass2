# Multi-Model Classification Framework

**Machine Learning Assignment 2**  
**Student ID:** 2025aa05325
**Name:** Gaikwad Abhinav Rajaram
**Date:** February 15, 2026

---

## ğŸ“Š Project Overview

A professional machine learning framework that implements and compares **6 different classification algorithms** on any binary/multi-class dataset. The system provides automated model training, comprehensive evaluation metrics, and an interactive web interface for real-time predictions.

### ğŸ¯ Key Features

- **Multi-Model Comparison** - Train and compare 6 ML algorithms simultaneously
- **Comprehensive Metrics** - Evaluate models using 6 industry-standard metrics
- **Interactive Web UI** - Upload any dataset and get instant predictions with visualizations
- **Production-Ready Code** - Professional package structure with proper separation of concerns
- **Automated Pipeline** - End-to-end workflow from data loading to model deployment
- **Flexible & Extensible** - Easily add new models or metrics

---

## ğŸ¤– Implemented Algorithms

The framework includes 6 popular classification algorithms covering different ML paradigms:

| Algorithm | Type | Strengths |
|-----------|------|-----------|
| **Logistic Regression** | Linear | Fast, interpretable, probabilistic |
| **Decision Tree** | Tree-based | Non-linear, interpretable |
| **K-Nearest Neighbors** | Instance-based | No training phase, flexible |
| **Naive Bayes** | Probabilistic | Fast, works with small data |
| **Random Forest** | Ensemble | Robust, handles overfitting |
| **XGBoost** | Gradient Boosting | State-of-art, high performance |

---

## ğŸ“ˆ Evaluation Metrics

Each model is evaluated using **6 comprehensive metrics**:

1. **Accuracy** - Overall correctness of predictions
2. **AUC-ROC** - Area Under the ROC Curve (ranking quality)
3. **Precision** - Positive predictive value (TP / TP+FP)
4. **Recall** - Sensitivity, true positive rate (TP / TP+FN)
5. **F1-Score** - Harmonic mean of precision and recall
6. **MCC** - Matthews Correlation Coefficient (balanced measure)

---

## ğŸ¨ Interactive Web Application

### Features

- **ğŸ“ Upload Any Dataset** - CSV files with features and optional labels
- **ğŸ¯ Model Selection** - Choose from 6 trained algorithms
- **ğŸ“Š Real-Time Predictions** - Get instant classification results
- **ğŸ“‰ Visual Analytics** - Confusion matrix, performance charts
- **ğŸ“‹ Detailed Reports** - Classification report with per-class metrics
- **ğŸ’¾ Export Results** - Download predictions as CSV

### Screenshots



---

## ğŸ—ï¸ Architecture & Design

### Professional Package Structure
```
breast-cancer-classification/
â”œâ”€â”€ com/abhi/ml/src/           # Main application package
â”‚   â”œâ”€â”€ config/                # Configuration management
â”‚   â”‚   â””â”€â”€ settings.py        # Centralized settings & constants
â”‚   â”œâ”€â”€ data/                  # Data handling layer
â”‚   â”‚   â”œâ”€â”€ loader.py          # Dataset loading utilities
â”‚   â”‚   â””â”€â”€ preprocessor.py   # Feature scaling & preprocessing
â”‚   â”œâ”€â”€ models/                # ML models layer
â”‚   â”‚   â”œâ”€â”€ base_model.py      # Abstract base class
â”‚   â”‚   â”œâ”€â”€ logistic_model.py  # Individual model implementations
â”‚   â”‚   â”œâ”€â”€ decision_tree_model.py
â”‚   â”‚   â”œâ”€â”€ knn_model.py
â”‚   â”‚   â”œâ”€â”€ naive_bayes_model.py
â”‚   â”‚   â”œâ”€â”€ random_forest_model.py
â”‚   â”‚   â””â”€â”€ xgboost_model.py
â”‚   â”œâ”€â”€ evaluation/            # Model evaluation
â”‚   â”‚   â””â”€â”€ metrics.py         # Metrics calculation & reporting
â”‚   â”œâ”€â”€ utils/                 # Shared utilities
â”‚   â”‚   â”œâ”€â”€ file_handler.py    # File I/O operations
â”‚   â”‚   â””â”€â”€ logger.py          # Logging utilities
â”‚   â””â”€â”€ main.py               # Training pipeline orchestrator
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ data/                 # Generated datasets
â”‚   â””â”€â”€ models/               # Trained model artifacts (.pkl)
â”œâ”€â”€ app.py                    # Streamlit web application
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # Documentation
```

### Design Principles

- **Separation of Concerns** - Clear layer separation (data, models, evaluation, utils)
- **Abstract Base Class Pattern** - Consistent interface for all models
- **Dependency Injection** - Flexible configuration management
- **Single Responsibility** - Each module has one clear purpose
- **DRY Principle** - Reusable components throughout

---

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/abhe9v/2025aa05325_ml_ass2.git
cd 2025aa05325_ml_ass2

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Train Models
```bash
# Run complete training pipeline
python -m com.abhi.ml.src.main
```

**Output:**
- Trains all 6 models
- Generates performance metrics
- Saves models to `resources/models/`
- Creates test dataset in `resources/data/`

### Launch Web Application
```bash
streamlit run app.py
```

Opens at `http://localhost:8501`

---

## ğŸ’¡ How It Works

### 1. **Data Pipeline**
```python
DataLoader â†’ Preprocessing â†’ Train/Test Split â†’ Feature Scaling
```

### 2. **Model Training**
```python
For each of 6 models:
    â”œâ”€â”€ Initialize with config
    â”œâ”€â”€ Train on scaled data
    â”œâ”€â”€ Generate predictions
    â”œâ”€â”€ Calculate 6 metrics
    â””â”€â”€ Save model artifact
```

### 3. **Web Interface**
```python
User uploads CSV â†’ Model loads from .pkl â†’ Predictions â†’ Visualizations
```

---

## ğŸ“Š Usage Example

### Using Your Own Dataset

**Requirements:**
- CSV format with numeric features
- Optional: Include `target` column for evaluation
- Recommended: â‰¥500 samples, â‰¥12 features

**Steps:**

1. **Prepare Your Data**
   - Ensure your CSV has appropriate feature `columns` and `target` column for labels

2. **Upload to Web App**
   - Select model from dropdown
   - Upload CSV file
   - Click "Run Predictions"

3. **View Results**
   - Performance metrics (if labels included)
   - Confusion matrix visualization
   - Prediction confidence scores
   - Download results as CSV

---

## ğŸ”§ Customization

### Add a New Model
Add a new model by creating a class that inherits from `BaseMLModel` and implementing the `build_model` method.
Example:
```python
# Create: com/abhi/ml/src/models/your_model.py
from com.abhi.ml.src.models.base_model import BaseMLModel
from sklearn.svm import SVC

class SVMModel(BaseMLModel):
    def __init__(self):
        super().__init__(model_name="SVM")
    
    def build_model(self):
        return SVC(kernel='rbf', probability=True)
```

### Add a New Metric
Add a new metric by importing it from `sklearn.metrics` and including it in the `calculate_metrics` function.
Example:
```python
# Edit: com/abhi/ml/src/evaluation/metrics.py
from sklearn.metrics import your_metric

def calculate_metrics(...):
    metrics = {
        # ... existing metrics
        'YourMetric': your_metric(y_true, y_pred)
    }
```

---

## ğŸ“¦ Dependencies
```txt
streamlit>=1.28.0          # Web UI framework
scikit-learn>=1.3.0        # ML algorithms & metrics
numpy>=1.24.0,<2.0.0       # Numerical computing
pandas>=2.0.0              # Data manipulation
matplotlib>=3.9.0          # Visualization
seaborn>=0.12.0            # Statistical visualization
scipy>=1.10.0              # Scientific computing
```

---

## ğŸ“ˆ Performance Optimization

The framework includes:
- **Efficient Preprocessing** - One-time scaling, reusable scaler
- **Model Persistence** - Trained models saved as .pkl files
- **Batch Predictions** - Handle multiple samples efficiently
- **Caching** - Streamlit caching for faster UI response
- **Logging** - Track training progress and errors

---

## ğŸ”¬ Technical Highlights

### Code Quality
- âœ… Professional package structure
- âœ… Type hints for better IDE support
- âœ… Comprehensive logging
- âœ… Error handling throughout
- âœ… Modular and maintainable

### Best Practices
- âœ… Separation of concerns
- âœ… DRY (Don't Repeat Yourself)
- âœ… SOLID principles
- âœ… Configuration management
- âœ… Consistent coding style

---

## ğŸ“ Assignment Compliance

**Requirements Met:**

âœ… **6 Classification Algorithms** - Logistic Regression, Decision Tree, kNN, Naive Bayes, Random Forest, XGBoost  
âœ… **6 Evaluation Metrics** - Accuracy, AUC, Precision, Recall, F1-Score, MCC  
âœ… **Dataset Requirements** - Tested on 569 samples with 30 features  
âœ… **GitHub Repository** - Complete source code with professional structure  
âœ… **Streamlit Application** - Interactive web UI for predictions  
âœ… **Comprehensive Documentation** - README with architecture and usage guide  


---



## ğŸ“„ License

This project is submitted as part of academic coursework at BITS Pilani.

---

## ğŸ™ Acknowledgments

- BITS Pilani faculty for guidance
- scikit-learn team for excellent ML libraries
- Streamlit team for the intuitive web framework
- Open-source ML community

---